{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9fe45fe-ba99-4f82-920d-f41f33cbdf3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-02T12:35:30.167041Z",
     "iopub.status.busy": "2023-01-02T12:35:30.166813Z",
     "iopub.status.idle": "2023-01-02T12:35:54.983928Z",
     "shell.execute_reply": "2023-01-02T12:35:54.983440Z",
     "shell.execute_reply.started": "2023-01-02T12:35:30.167016Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e54cf61a20d4d1c9214bf1257d4a290",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>2</td><td>application_1672659663072_0004</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-20-0-4-191.ap-south-1.compute.internal:20888/proxy/application_1672659663072_0004/\" class=\"emr-proxy-link\" emr-resource=\"j-2HL2JQ6K3Q0F9\n",
       "\" application-id=\"application_1672659663072_0004\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-20-0-4-209.ap-south-1.compute.internal:8042/node/containerlogs/container_1672659663072_0004_01_000001/livy\" >Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "import boto3\n",
    "import sys\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "import configparser\n",
    "\n",
    "DataFrameName = \"128 WPS\"\n",
    "JobName = \"128 WPS (Deviation Above G/S)\"\n",
    "\n",
    "region_name = \"ap-south-1\"\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a53b86b-9362-40bb-a56a-77056089b0a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-02T12:45:34.207934Z",
     "iopub.status.busy": "2023-01-02T12:45:34.207699Z",
     "iopub.status.idle": "2023-01-02T12:47:37.805451Z",
     "shell.execute_reply": "2023-01-02T12:47:37.804716Z",
     "shell.execute_reply.started": "2023-01-02T12:45:34.207911Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d653cbaca5a4c7ea3bec22f059fa65b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "FlightDataInfo = spark \\\n",
    ".read \\\n",
    ".format(\"csv\") \\\n",
    ".option(\"header\", \"true\") \\\n",
    ".load('s3://sjet-datamart-bucket/precog/precog-file-info/q400/128-wps/20221231/')\n",
    "\n",
    "FlightDataInfo.select('Year', 'Month').distinct().createOrReplaceTempView('FlightDataInfo')\n",
    "      \n",
    "Q400Final = spark.sql(f'''\n",
    "                    SELECT   *\n",
    "                                FROM precogdb.q400128wps\n",
    "                                WHERE CONCAT(Year,Month) = (SELECT  CONCAT(Year,Month)\n",
    "                                                        FROM FlightDataInfo )\n",
    "                                                        AND CAST(loadeddate AS DATE) = CAST(current_timestamp() AS DATE)\n",
    "                                AND ECN <> ''\n",
    "            AND DesigFlightNumber <> ''\n",
    "            \n",
    "              ''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af36d65b-9172-4562-bcba-6560e4ee5a4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-02T12:47:37.806953Z",
     "iopub.status.busy": "2023-01-02T12:47:37.806708Z",
     "iopub.status.idle": "2023-01-02T12:47:39.070972Z",
     "shell.execute_reply": "2023-01-02T12:47:39.070492Z",
     "shell.execute_reply.started": "2023-01-02T12:47:37.806920Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b43b859939ad427092fb59cf2a4fe05d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o122.count.\n",
      ": org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\n",
      "Exchange SinglePartition, ENSURE_REQUIREMENTS, [id=#427]\n",
      "+- *(1) HashAggregate(keys=[], functions=[partial_count(1)], output=[count#876L])\n",
      "   +- *(1) Project\n",
      "      +- *(1) Filter (((((isnotnull(loadeddate#715) AND isnotnull(ECN#653)) AND isnotnull(DesigFlightNumber#647)) AND (cast(loadeddate#715 as date) = 19359)) AND NOT (ECN#653 = )) AND NOT (DesigFlightNumber#647 = ))\n",
      "         +- *(1) ColumnarToRow\n",
      "            +- FileScan parquet precogdb.q400128wps[desigflightnumber#647,ecn#653,loadeddate#715,year#719,month#720] Batched: true, DataFilters: [isnotnull(loadeddate#715), isnotnull(ecn#653), isnotnull(desigflightnumber#647), (cast(loadeddat..., Format: Parquet, Location: CatalogFileIndex[s3://sjet-datamart-bucket/precog/precog-processed-data/q400/q400128wps], PartitionFilters: [(concat(year#719, month#720) = Subquery subquery#645, [id=#392])], PushedFilters: [IsNotNull(loadeddate), IsNotNull(ecn), IsNotNull(desigflightnumber), Not(EqualTo(ecn,)), Not(Equ..., ReadSchema: struct<desigflightnumber:string,ecn:string,loadeddate:timestamp>\n",
      "                  +- Subquery subquery#645, [id=#392]\n",
      "                     +- AdaptiveSparkPlan isFinalPlan=true\n",
      "                        +- == Final Plan ==\n",
      "                           *(3) HashAggregate(keys=[Year#633, Month#634], functions=[], output=[concat(Year, Month)#721])\n",
      "                           +- CustomShuffleReader coalesced\n",
      "                              +- ShuffleQueryStage 1\n",
      "                                 +- Exchange hashpartitioning(Year#633, Month#634, 1000), ENSURE_REQUIREMENTS, [id=#442]\n",
      "                                    +- *(2) HashAggregate(keys=[Year#633, Month#634], functions=[], output=[Year#633, Month#634])\n",
      "                                       +- FileScan csv [year#633,month#634] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[s3://sjet-datamart-bucket/precog/precog-file-info/q400/128-wps/20221231], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<year:string,month:string>\n",
      "                        +- == Initial Plan ==\n",
      "                           HashAggregate(keys=[Year#633, Month#634], functions=[], output=[concat(Year, Month)#721])\n",
      "                           +- Exchange hashpartitioning(Year#633, Month#634, 1000), ENSURE_REQUIREMENTS, [id=#389]\n",
      "                              +- HashAggregate(keys=[Year#633, Month#634], functions=[], output=[Year#633, Month#634])\n",
      "                                 +- FileScan csv [year#633,month#634] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[s3://sjet-datamart-bucket/precog/precog-file-info/q400/128-wps/20221231], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<year:string,month:string>\n",
      "\n",
      "\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.$anonfun$materializeFuture$1(ShuffleExchangeExec.scala:101)\n",
      "\tat org.apache.spark.sql.util.LazyValue.getOrInit(LazyValue.scala:41)\n",
      "\tat org.apache.spark.sql.execution.exchange.Exchange.getOrInitMaterializeFuture(Exchange.scala:71)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.materializeFuture(ShuffleExchangeExec.scala:97)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.materialize(ShuffleExchangeExec.scala:85)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.materialize$(ShuffleExchangeExec.scala:84)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.materialize(ShuffleExchangeExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.doMaterialize(QueryStageExec.scala:161)\n",
      "\tat org.apache.spark.sql.execution.adaptive.QueryStageExec.$anonfun$materialize$1(QueryStageExec.scala:74)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:232)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:229)\n",
      "\tat org.apache.spark.sql.execution.adaptive.QueryStageExec.materialize(QueryStageExec.scala:74)\n",
      "\tat org.apache.spark.sql.execution.adaptive.MaterializeExecutable.tryStart(AdaptiveExecutable.scala:396)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutorRuntime.startChild(AdaptiveExecutor.scala:225)\n",
      "\tat org.apache.spark.sql.execution.adaptive.ExecutionHelper.start(ExecutionHelper.scala:47)\n",
      "\tat org.apache.spark.sql.execution.adaptive.QueryStageExecutable$$anon$2.$anonfun$new$1(AdaptiveExecutable.scala:251)\n",
      "\tat org.apache.spark.sql.execution.adaptive.ExecutionHelper$Listener.$anonfun$onChildSuccess$2(ExecutionHelper.scala:55)\n",
      "\tat org.apache.spark.sql.execution.adaptive.ExecutionHelper$Listener.$anonfun$onChildSuccess$2$adapted(ExecutionHelper.scala:54)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.sql.execution.adaptive.ExecutionHelper$Listener.$anonfun$onChildSuccess$1(ExecutionHelper.scala:54)\n",
      "\tat org.apache.spark.sql.execution.adaptive.ExecutionHelper$Listener.$anonfun$onChildSuccess$1$adapted(ExecutionHelper.scala:53)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.sql.execution.adaptive.ExecutionHelper$Listener.onChildSuccess(ExecutionHelper.scala:53)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutorRuntime.$anonfun$onActiveChildSuccess$2(AdaptiveExecutor.scala:314)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutorRuntime.$anonfun$onActiveChildSuccess$2$adapted(AdaptiveExecutor.scala:314)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutorRuntime.onActiveChildSuccess(AdaptiveExecutor.scala:314)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutorRuntime.onChildSuccess(AdaptiveExecutor.scala:284)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.$anonfun$doRun$1(AdaptiveExecutor.scala:92)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.$anonfun$doRun$1$adapted(AdaptiveExecutor.scala:91)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.doRun(AdaptiveExecutor.scala:91)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.tryRunningAndGetFuture(AdaptiveExecutor.scala:66)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.execute(AdaptiveExecutor.scala:57)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:184)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:183)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:405)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$count$1(Dataset.scala:3047)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$count$1$adapted(Dataset.scala:3046)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3751)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3749)\n",
      "\tat org.apache.spark.sql.Dataset.count(Dataset.scala:3046)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.lang.RuntimeException: more than one row returned by a subquery used as an expression:\n",
      "Subquery subquery#645, [id=#392]\n",
      "+- AdaptiveSparkPlan isFinalPlan=true\n",
      "   +- == Final Plan ==\n",
      "      *(3) HashAggregate(keys=[Year#633, Month#634], functions=[], output=[concat(Year, Month)#721])\n",
      "      +- CustomShuffleReader coalesced\n",
      "         +- ShuffleQueryStage 1\n",
      "            +- Exchange hashpartitioning(Year#633, Month#634, 1000), ENSURE_REQUIREMENTS, [id=#442]\n",
      "               +- *(2) HashAggregate(keys=[Year#633, Month#634], functions=[], output=[Year#633, Month#634])\n",
      "                  +- FileScan csv [year#633,month#634] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[s3://sjet-datamart-bucket/precog/precog-file-info/q400/128-wps/20221231], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<year:string,month:string>\n",
      "   +- == Initial Plan ==\n",
      "      HashAggregate(keys=[Year#633, Month#634], functions=[], output=[concat(Year, Month)#721])\n",
      "      +- Exchange hashpartitioning(Year#633, Month#634, 1000), ENSURE_REQUIREMENTS, [id=#389]\n",
      "         +- HashAggregate(keys=[Year#633, Month#634], functions=[], output=[Year#633, Month#634])\n",
      "            +- FileScan csv [year#633,month#634] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[s3://sjet-datamart-bucket/precog/precog-file-info/q400/128-wps/20221231], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<year:string,month:string>\n",
      "\n",
      "\tat scala.sys.package$.error(package.scala:30)\n",
      "\tat org.apache.spark.sql.execution.ScalarSubquery.updateResult(subquery.scala:103)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$waitForSubqueries$1(SparkPlan.scala:262)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$waitForSubqueries$1$adapted(SparkPlan.scala:261)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.waitForSubqueries(SparkPlan.scala:261)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:231)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:229)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeColumnar(SparkPlan.scala:217)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.doExecuteColumnar(WholeStageCodegenExec.scala:585)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeColumnar$1(SparkPlan.scala:221)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:232)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:229)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeColumnar(SparkPlan.scala:217)\n",
      "\tat org.apache.spark.sql.execution.ColumnarToRowExec.inputRDDs(Columnar.scala:202)\n",
      "\tat org.apache.spark.sql.execution.FilterExec.inputRDDs(basicPhysicalOperators.scala:194)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:50)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:137)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:823)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:194)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:232)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:229)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:190)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:171)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:171)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture$lzycompute(ShuffleExchangeExec.scala:175)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture(ShuffleExchangeExec.scala:174)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.$anonfun$materializeFuture$2(ShuffleExchangeExec.scala:101)\n",
      "\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n",
      "\t... 74 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", line 665, in count\n",
      "    return int(self._jdf.count())\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1305, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 111, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o122.count.\n",
      ": org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\n",
      "Exchange SinglePartition, ENSURE_REQUIREMENTS, [id=#427]\n",
      "+- *(1) HashAggregate(keys=[], functions=[partial_count(1)], output=[count#876L])\n",
      "   +- *(1) Project\n",
      "      +- *(1) Filter (((((isnotnull(loadeddate#715) AND isnotnull(ECN#653)) AND isnotnull(DesigFlightNumber#647)) AND (cast(loadeddate#715 as date) = 19359)) AND NOT (ECN#653 = )) AND NOT (DesigFlightNumber#647 = ))\n",
      "         +- *(1) ColumnarToRow\n",
      "            +- FileScan parquet precogdb.q400128wps[desigflightnumber#647,ecn#653,loadeddate#715,year#719,month#720] Batched: true, DataFilters: [isnotnull(loadeddate#715), isnotnull(ecn#653), isnotnull(desigflightnumber#647), (cast(loadeddat..., Format: Parquet, Location: CatalogFileIndex[s3://sjet-datamart-bucket/precog/precog-processed-data/q400/q400128wps], PartitionFilters: [(concat(year#719, month#720) = Subquery subquery#645, [id=#392])], PushedFilters: [IsNotNull(loadeddate), IsNotNull(ecn), IsNotNull(desigflightnumber), Not(EqualTo(ecn,)), Not(Equ..., ReadSchema: struct<desigflightnumber:string,ecn:string,loadeddate:timestamp>\n",
      "                  +- Subquery subquery#645, [id=#392]\n",
      "                     +- AdaptiveSparkPlan isFinalPlan=true\n",
      "                        +- == Final Plan ==\n",
      "                           *(3) HashAggregate(keys=[Year#633, Month#634], functions=[], output=[concat(Year, Month)#721])\n",
      "                           +- CustomShuffleReader coalesced\n",
      "                              +- ShuffleQueryStage 1\n",
      "                                 +- Exchange hashpartitioning(Year#633, Month#634, 1000), ENSURE_REQUIREMENTS, [id=#442]\n",
      "                                    +- *(2) HashAggregate(keys=[Year#633, Month#634], functions=[], output=[Year#633, Month#634])\n",
      "                                       +- FileScan csv [year#633,month#634] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[s3://sjet-datamart-bucket/precog/precog-file-info/q400/128-wps/20221231], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<year:string,month:string>\n",
      "                        +- == Initial Plan ==\n",
      "                           HashAggregate(keys=[Year#633, Month#634], functions=[], output=[concat(Year, Month)#721])\n",
      "                           +- Exchange hashpartitioning(Year#633, Month#634, 1000), ENSURE_REQUIREMENTS, [id=#389]\n",
      "                              +- HashAggregate(keys=[Year#633, Month#634], functions=[], output=[Year#633, Month#634])\n",
      "                                 +- FileScan csv [year#633,month#634] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[s3://sjet-datamart-bucket/precog/precog-file-info/q400/128-wps/20221231], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<year:string,month:string>\n",
      "\n",
      "\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.$anonfun$materializeFuture$1(ShuffleExchangeExec.scala:101)\n",
      "\tat org.apache.spark.sql.util.LazyValue.getOrInit(LazyValue.scala:41)\n",
      "\tat org.apache.spark.sql.execution.exchange.Exchange.getOrInitMaterializeFuture(Exchange.scala:71)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.materializeFuture(ShuffleExchangeExec.scala:97)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.materialize(ShuffleExchangeExec.scala:85)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.materialize$(ShuffleExchangeExec.scala:84)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.materialize(ShuffleExchangeExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.doMaterialize(QueryStageExec.scala:161)\n",
      "\tat org.apache.spark.sql.execution.adaptive.QueryStageExec.$anonfun$materialize$1(QueryStageExec.scala:74)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:232)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:229)\n",
      "\tat org.apache.spark.sql.execution.adaptive.QueryStageExec.materialize(QueryStageExec.scala:74)\n",
      "\tat org.apache.spark.sql.execution.adaptive.MaterializeExecutable.tryStart(AdaptiveExecutable.scala:396)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutorRuntime.startChild(AdaptiveExecutor.scala:225)\n",
      "\tat org.apache.spark.sql.execution.adaptive.ExecutionHelper.start(ExecutionHelper.scala:47)\n",
      "\tat org.apache.spark.sql.execution.adaptive.QueryStageExecutable$$anon$2.$anonfun$new$1(AdaptiveExecutable.scala:251)\n",
      "\tat org.apache.spark.sql.execution.adaptive.ExecutionHelper$Listener.$anonfun$onChildSuccess$2(ExecutionHelper.scala:55)\n",
      "\tat org.apache.spark.sql.execution.adaptive.ExecutionHelper$Listener.$anonfun$onChildSuccess$2$adapted(ExecutionHelper.scala:54)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.sql.execution.adaptive.ExecutionHelper$Listener.$anonfun$onChildSuccess$1(ExecutionHelper.scala:54)\n",
      "\tat org.apache.spark.sql.execution.adaptive.ExecutionHelper$Listener.$anonfun$onChildSuccess$1$adapted(ExecutionHelper.scala:53)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.sql.execution.adaptive.ExecutionHelper$Listener.onChildSuccess(ExecutionHelper.scala:53)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutorRuntime.$anonfun$onActiveChildSuccess$2(AdaptiveExecutor.scala:314)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutorRuntime.$anonfun$onActiveChildSuccess$2$adapted(AdaptiveExecutor.scala:314)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutorRuntime.onActiveChildSuccess(AdaptiveExecutor.scala:314)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutorRuntime.onChildSuccess(AdaptiveExecutor.scala:284)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.$anonfun$doRun$1(AdaptiveExecutor.scala:92)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.$anonfun$doRun$1$adapted(AdaptiveExecutor.scala:91)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.doRun(AdaptiveExecutor.scala:91)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.tryRunningAndGetFuture(AdaptiveExecutor.scala:66)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.execute(AdaptiveExecutor.scala:57)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:184)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:183)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:405)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$count$1(Dataset.scala:3047)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$count$1$adapted(Dataset.scala:3046)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3751)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3749)\n",
      "\tat org.apache.spark.sql.Dataset.count(Dataset.scala:3046)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.lang.RuntimeException: more than one row returned by a subquery used as an expression:\n",
      "Subquery subquery#645, [id=#392]\n",
      "+- AdaptiveSparkPlan isFinalPlan=true\n",
      "   +- == Final Plan ==\n",
      "      *(3) HashAggregate(keys=[Year#633, Month#634], functions=[], output=[concat(Year, Month)#721])\n",
      "      +- CustomShuffleReader coalesced\n",
      "         +- ShuffleQueryStage 1\n",
      "            +- Exchange hashpartitioning(Year#633, Month#634, 1000), ENSURE_REQUIREMENTS, [id=#442]\n",
      "               +- *(2) HashAggregate(keys=[Year#633, Month#634], functions=[], output=[Year#633, Month#634])\n",
      "                  +- FileScan csv [year#633,month#634] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[s3://sjet-datamart-bucket/precog/precog-file-info/q400/128-wps/20221231], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<year:string,month:string>\n",
      "   +- == Initial Plan ==\n",
      "      HashAggregate(keys=[Year#633, Month#634], functions=[], output=[concat(Year, Month)#721])\n",
      "      +- Exchange hashpartitioning(Year#633, Month#634, 1000), ENSURE_REQUIREMENTS, [id=#389]\n",
      "         +- HashAggregate(keys=[Year#633, Month#634], functions=[], output=[Year#633, Month#634])\n",
      "            +- FileScan csv [year#633,month#634] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[s3://sjet-datamart-bucket/precog/precog-file-info/q400/128-wps/20221231], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<year:string,month:string>\n",
      "\n",
      "\tat scala.sys.package$.error(package.scala:30)\n",
      "\tat org.apache.spark.sql.execution.ScalarSubquery.updateResult(subquery.scala:103)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$waitForSubqueries$1(SparkPlan.scala:262)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$waitForSubqueries$1$adapted(SparkPlan.scala:261)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.waitForSubqueries(SparkPlan.scala:261)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:231)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:229)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeColumnar(SparkPlan.scala:217)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.doExecuteColumnar(WholeStageCodegenExec.scala:585)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeColumnar$1(SparkPlan.scala:221)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:232)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:229)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeColumnar(SparkPlan.scala:217)\n",
      "\tat org.apache.spark.sql.execution.ColumnarToRowExec.inputRDDs(Columnar.scala:202)\n",
      "\tat org.apache.spark.sql.execution.FilterExec.inputRDDs(basicPhysicalOperators.scala:194)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:50)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:137)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:823)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:194)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:232)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:229)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:190)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:171)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:171)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture$lzycompute(ShuffleExchangeExec.scala:175)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture(ShuffleExchangeExec.scala:174)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.$anonfun$materializeFuture$2(ShuffleExchangeExec.scala:101)\n",
      "\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n",
      "\t... 74 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Q400Final.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b57c3c-091a-4b15-8084-86f7c94740fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
